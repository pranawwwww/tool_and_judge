This project is about LLM as judge. It explores the relationship between LLM's perplexity of an answer and its preference of it.

Specifically, we collect two kinds of data:
1. For each question and two answers, prompt the LLM to decide which answer is better, and record this information.
2. Let the same LLM answer this question, but during generation, we only collect the probability distribution of the next token but assign the actual token with the one in the given answer, and then calculate the log perplexity to be the average log probability of [each token to be generated by LLM equals the given answer's token]. In this way, we get one perplexity metric for each of the answer, and compare them.

Then, we compare the two kinds of data line by line and calculate how correlated the data is.

We start from gpt-4o-mini model. Assume the API key is in .env folder.

The dataset is loaded through:
'''
from datasets import load_dataset
ds = load_dataset("willchow66/mmmlu-intersection-filtered", "ar_xy")
'''

The dataset structure is not determined yet. Print the first few lines to see what it looks like.


Current progress: trying to figure out the chat template, where to cut to retrieve the full text and the question text
See if there is ending special tokens

Feed the full text to model.forward, and grab the probability from start token to end token.


In the last meeting, we decided to do right / wrong pair. 
I've got the strategy for choosing the right / wrong pair.
Then let the model decide which is correct. (Immediately?) do both prompts.

If the judgment is wrong, then do another completely opposite experiment to check if the error is due to bias or capability

Probabilistic mode: measure the probability difference between completely opposite experiments

zh correct, en wrong; zh wrong, en correct; ...

fix the order.


Probability difference (calibration)

Needs to split subjects?

Experiments: 2 (zh correct en incorrect, etc.) * 3 (perplexity, preference direct, preference thinking)

Data collected: accuracy, correlation (pearson correlation), bias (binary, continuous)

depends on subject

