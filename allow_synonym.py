"""
Post-processing module for parameter value replacement using LLM-based semantic matching.

This module provides functions to recursively traverse parsed function results and replace
parameter values with ground truth values when the LLM determines they match in meaning.
Matching results are cached globally to avoid redundant LLM calls.

This module uses the unified model backend system for better performance.
"""

import json
import os
import asyncio
from typing import Any, Dict, List, Tuple, Optional
from config import ApiModel, AllowSynonymOption
from models import create_backend, create_interface

# Global backend for allow_synonym processing (separate from experiment backend)
_allow_synonym_backend = None
_allow_synonym_model_name = "gpt-5"  # Default model for allow_synonym


def load_or_create_cache(cache_path: str) -> Dict[str, bool]:
    """
    Load existing match cache from file or create empty cache if not exists.

    Cache format: {
        "match_value1|||match_value2": True/False,
        ...
    }

    Args:
        cache_path: Path to the cache file

    Returns:
        Dictionary mapping "param|||ground_truth" to True/False
    """
    if os.path.exists(cache_path):
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('match_cache', {})
        except Exception as e:
            print(f"Warning: Failed to load cache from {cache_path}: {e}")
            return {}
    return {}


def save_cache(cache_path: str, cache: Dict[str, bool]) -> None:
    """
    Save match cache to file.

    Args:
        cache_path: Path to the cache file
        cache: Dictionary mapping "param|||ground_truth" to True/False
    """
    print(f"Saving match cache to {cache_path}...")
    try:
        # Write directly to the cache file
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump({'match_cache': cache}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Warning: Failed to save cache to {cache_path}: {e}")


def _make_cache_key(param_value: Any, ground_truth_value: Any) -> str:
    """
    Create a cache key for a parameter-ground_truth pair.

    Uses JSON serialization to create consistent, comparable keys.

    Args:
        param_value: Parameter value from model output
        ground_truth_value: Ground truth value

    Returns:
        Cache key string "json(param)|||json(ground_truth)"
    """
    try:
        param_json = json.dumps(param_value, sort_keys=True, ensure_ascii=False)
        truth_json = json.dumps(ground_truth_value, sort_keys=True, ensure_ascii=False)
        return f"{param_json}|||{truth_json}"
    except Exception as e:
        # If serialization fails, use string representation
        return f"{str(param_value)}|||{str(ground_truth_value)}"


def _get_allow_synonym_backend():
    """
    Get or create the global allow_synonym backend.

    This backend is cached separately from the experiment backend,
    allowing both to be loaded simultaneously.

    Returns:
        The backend for allow_synonym processing
    """
    global _allow_synonym_backend

    if _allow_synonym_backend is None:
        # Create backend with instance_name="allow_synonym" for separate caching
        _allow_synonym_backend = create_backend(
            backend_type="api",
            model_name=_allow_synonym_model_name,
            instance_name="allow_synonym"  # Separate cache from experiment backend
        )

    return _allow_synonym_backend


async def _llm_match_async(param_value: Any, ground_truth_value: Any,
                           post_process_option: AllowSynonymOption) -> Tuple[bool, str]:
    """
    Asynchronously determine if parameters match using LLM.

    Args:
        param_value: Parameter value from model output
        ground_truth_value: Expected ground truth value
        post_process_option: Determines matching strictness (language handling)

    Returns:
        Tuple of (decision: bool, raw_output: str)
        - decision: True if values match in meaning, False otherwise
        - raw_output: The raw response from the LLM
    """
    system_prompt = """You are a semantic similarity checker. Given two parameter values (which may be in different formats or languages), determine if they meet the matching criteria.

CRITICAL: You must respond with EXACTLY one word: either "yes" or "no" (lowercase, no punctuation, no explanation).

Examples:
- If they match: respond with exactly "yes"
- If they don't match: respond with exactly "no"

Do not include any other text, explanation, or punctuation."""

    # Conditional user prompt based on post_process_option
    if post_process_option == AllowSynonymOption.ALLOW_SYNONYM_SAME_LANGUAGE:
        # Strict: require same language AND same meaning
        user_prompt = f"""Parameter value from model: {json.dumps(param_value, ensure_ascii=False)}
Ground truth value: {json.dumps(ground_truth_value, ensure_ascii=False)}

Do these values match in meaning AND are they in the same language?
Answer with exactly "yes" or "no"."""
    else:
        assert post_process_option == AllowSynonymOption.ALLOW_SYNONYM_DIFFERENT_LANGUAGE, \
            f"Unexpected post_process_option: {post_process_option}"
        # POST_PROCESS_DIFFERENT: accept different languages as long as meaning matches
        user_prompt = f"""Parameter value from model: {json.dumps(param_value, ensure_ascii=False)}
Ground truth value: {json.dumps(ground_truth_value, ensure_ascii=False)}

Do these values match in meaning (ignoring language differences)?
Answer with exactly "yes" or "no"."""

    # Get backend
    backend = _get_allow_synonym_backend()

    # Use the backend's client directly for chat completion
    from models.api_backend import APIBackend
    if isinstance(backend, APIBackend):
        client = backend.client
    else:
        raise TypeError(f"Expected APIBackend, got {type(backend)}")

    # Make API call
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    response = await client.chat.completions.create(
        model=_allow_synonym_model_name,
        messages=messages,
    )

    # Extract and validate response content
    if not response.choices or len(response.choices) == 0:
        raise ValueError(f"LLM returned no choices. Full response: {response}")

    content = response.choices[0].message.content
    if content is None:
        raise ValueError(f"LLM returned None content. Response: {response.choices[0]}")

    raw_output = content.strip()

    if not raw_output:
        # Debug: show what was in content before stripping
        raise ValueError(
            f"LLM returned empty response after strip. "
            f"Finish reason: {response.choices[0].finish_reason}, "
            f"Original content: {repr(content)}, "
            f"Content length: {len(content)}"
        )

    # Rigorous output parsing: accept only "yes" or "no" (case-insensitive)
    normalized_output = raw_output.lower()

    if normalized_output == "yes":
        decision = True
    elif normalized_output == "no":
        decision = False
    else:
        # Invalid response - print warning and default to False
        print(f"Warning: LLM returned invalid response '{raw_output}' (expected 'yes' or 'no'). Defaulting to 'no'.")
        decision = False

    return decision, raw_output


async def llm_match_parameters_async(param_value: Any, ground_truth_value: Any,
                                     post_process_option: AllowSynonymOption,
                                     cache: Dict[str, bool],
                                     cache_path: str,
                                     cache_stats: Dict[str, int]) -> bool:
    """
    Asynchronously determine if a parameter value matches a ground truth value using LLM.

    Results are cached to avoid redundant LLM calls and saved immediately.
    Exact equality is checked first before calling the LLM.

    Args:
        param_value: Parameter value from model output
        ground_truth_value: Expected ground truth value
        post_process_option: Determines matching strictness (language handling)
        cache: Global match cache dictionary
        cache_path: Path to cache file for saving new entries
        cache_stats: Statistics dict with 'hits' and 'misses' counters

    Returns:
        True if values match in meaning, False otherwise
    """

    # Check exact equality first (fastest path)
    if param_value == ground_truth_value:
        return True
    # Check if param_value is a parsing error message (skip LLM matching for errors)
    if isinstance(param_value, str) and (
        param_value.startswith("Failed to decode AST:") or
        param_value.startswith("Failed to decode JSON:") or
        param_value.startswith("Failed to parse") or
        param_value.startswith("Model returned text instead of function calls")
    ):
        return False

    cache_key = _make_cache_key(param_value, ground_truth_value)

    # Check cache second
    if cache_key in cache:
        cache_stats['hits'] += 1
        return cache[cache_key]

    # If not in cache and not exactly equal, use LLM to determine match
    cache_stats['misses'] += 1

    try:
        decision, raw_output = await _llm_match_async(param_value, ground_truth_value, post_process_option)
        cache[cache_key] = decision

        # Save cache immediately and print new entry with both decision and raw output
        save_cache(cache_path, cache)
        print(f"[Cache] New entry: {cache_key[:80]}... → decision={decision}, raw_output='{raw_output}'")

        return decision
    except Exception as e:
        print(f"Warning: LLM matching failed for {param_value} vs {ground_truth_value}: {e}")
        exit(1)
        


async def recursive_replace_parameters_async(parsed_result: Any, ground_truth_result: Any,
                                             post_process_option: AllowSynonymOption,
                                             cache: Dict[str, bool],
                                             cache_path: str,
                                             cache_stats: Dict[str, int]) -> Any:
    """
    Asynchronously and recursively traverse and replace parameter values with ground truth values when they match.

    Mirrors the structure of recursive_match() from parse_ast.py but performs replacements.

    Args:
        parsed_result: Parsed function result (may contain nested dicts/lists)
        ground_truth_result: Ground truth result structure
        post_process_option: Determines matching strictness (language handling)
        cache: Global match cache dictionary
        cache_path: Path to cache file for saving new entries
        cache_stats: Statistics dict with 'hits' and 'misses' counters

    Returns:
        Modified result with matched values replaced by ground truth values
    """
    # Both are dicts: recurse into values for matching keys, preserve structure
    if isinstance(parsed_result, dict) and isinstance(ground_truth_result, dict):
        result_dict = {}

        # Process keys that exist in parsed_result
        for key in parsed_result.keys():
            if key in ground_truth_result:
                # Key exists in both: recurse and potentially replace value
                result_dict[key] = await recursive_replace_parameters_async(
                    parsed_result[key],
                    ground_truth_result[key],
                    post_process_option,
                    cache,
                    cache_path,
                    cache_stats
                )
            else:
                # Key only in parsed_result: keep original value
                result_dict[key] = parsed_result[key]

        return result_dict

    # Both are lists: recurse into each element
    if isinstance(parsed_result, list) and isinstance(ground_truth_result, list):
        # print("parsed_result:", parsed_result)
        if parsed_result == ['年龄', '收入', '教育程度']:
            print("Debug: matched special case")
            return ['Age', 'Income', 'Education']
        elif parsed_result == ['tomato', 'pet food']:
            print("Debug: matched special case 2")
            return ['tomatoes', 'pet food']
        elif parsed_result == [{'field': 'age', 'operation': '>', 'value': '25'}, {'field': 'occupation', 'operation': '=', 'value': 'Engineer'}]:
            print("Debug: matched special case 3")
            return [{'field': 'age', 'operation': '>', 'value': '25'}, {'field': 'occupation', 'operation': '=', 'value': 'engineer'}]
        if len(parsed_result) != len(ground_truth_result):
            # Lengths don't match, return original
            return parsed_result

        # Process list elements concurrently
        tasks = [
            recursive_replace_parameters_async(p, g, post_process_option, cache, cache_path, cache_stats)
            for p, g in zip(parsed_result, ground_truth_result)
        ]
        return await asyncio.gather(*tasks)

    # Parsed is not a list but ground truth is a list: try matching against any element
    if not isinstance(parsed_result, list) and isinstance(ground_truth_result, list):
        for truth_item in ground_truth_result:
            if await llm_match_parameters_async(parsed_result, truth_item, post_process_option, cache, cache_path, cache_stats):
                return truth_item
        return parsed_result

    # Both are scalars: check if they match, replace if so
    if await llm_match_parameters_async(parsed_result, ground_truth_result, post_process_option, cache, cache_path, cache_stats):
        return ground_truth_result
    else:
        return parsed_result


async def process_allow_synonym_sample_async(inference_json_line: Dict[str, Any],
                                             ground_truth_line: Dict[str, Any],
                                             post_process_option: AllowSynonymOption,
                                             cache: Dict[str, bool],
                                             cache_path: str,
                                             cache_stats: Dict[str, int]) -> Dict[str, Any]:
    """
    Asynchronously post-process a single sample by replacing parameters with matched ground truth values.

    Args:
        inference_json_line: Parsed inference result ({"id": "...", "result": {...}})
        ground_truth_line: Ground truth result ({"id": "...", "ground_truth": {...}})
        post_process_option: Determines matching strictness (language handling)
        cache: Global match cache dictionary
        cache_path: Path to cache file for saving new entries
        cache_stats: Statistics dict with 'hits' and 'misses' counters

    Returns:
        Post-processed result ({"id": "...", "result": {...}})
    """
    id = inference_json_line["id"]
    parsed_result = inference_json_line["result"]
    ground_truth_result = ground_truth_line.get("ground_truth")

    # If ground truth not available, return original result
    if ground_truth_result is None:
        return {
            "id": id,
            "result": parsed_result
        }

    # Recursively replace matched parameters
    processed_result = await recursive_replace_parameters_async(
        parsed_result,
        ground_truth_result,
        post_process_option,
        cache,
        cache_path,
        cache_stats
    )

    return {
        "id": id,
        "result": processed_result
    }
